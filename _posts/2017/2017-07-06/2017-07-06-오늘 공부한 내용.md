---
layout: post
title: "[공부] Hidden layer, Xavier, Dropout"
output: html_document
category: Hidden layer, Xavier, Dropout
---

# Softmax

## Multi-variable classification

    일단 softmax는 이런뜻이다.
    `logits = tf.matmul(X, W) + b`
    `softmax = exp(logits) / reduce_sum(exp(logits), dim)`

    Tensforflow로 구현하면 다음과 같다.

```python
hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)
# Cross entropy cost/loss
cross_entropy = -tf.reduce_sum(Y * tf.log(hypothesis), axis = 1)
cost = tf.reduce_mean(cross_entropy)
```

    `softmax_cross_entropy_with_logits` 로 간단하게

```python
cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits,
                                                 labels=Y_one_hot)
cost = tf.reduce_mean(cost_i)
```

---


# Hidden layer

## deep하게 NN 만들기

    이건 input이 784개, ouput이 10개인 (MNIST) NN형태

```python
# weights & bias for nn layers
W = tf.Variable(tf.random_normal([784, 10]))
b = tf.Variable(tf.random_normal([10]))

hypothesis = tf.matmul(X, W) + b
```

    이건 hidden layer가 2개에 256개의 node를 가진
    NN의 형태

```python
# weights & bias for nn layers
W1 = tf.Variable(tf.random_normal([784, 256]))
b1 = tf.Variable(tf.random_normal([256]))
L1 = tf.nn.relu(tf.matmul(X, W1) + b1)

W2 = tf.Variable(tf.random_normal([256, 256]))
b2 = tf.Variable(tf.random_normal([256]))
L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)

W3 = tf.Variable(tf.random_normal([256, 10]))
b3 = tf.Variable(tf.random_normal([10]))
hypothesis = tf.matmul(L2, W3) + b3
```


---

# Xavier

## Weight 의 초기값을 잘 선택할 수 있을까?


```python
W = tf.Variable(tf.random_normal([784, 256]))
```
    Glorot et al. 2010 에 따르면
    이렇게 랜덤 노말로 weight값을 주지 말

```python
W = np.random.randn(fan_in, fan_out)/np.sqrt(fan_in)
```

    이렇게 입력 노드와 출력노드의 갯수를 이용해서 W값을 정해주자
    Tensorflow 함수를 이용하면 다음과 같다.
    `initializer = tf.contrib.layers.xavier_initializer()`

```python
W1 = tf.get_variable("W1", shape=[784, 256],
                     initializer = tf.contrib.layers.xavier_initializer())
```

    He et al. 2015 에 따르면
    마지막 fan_in 을 2로 나눈값을 사용하는게 더 좋다.

```python
W = np.random.randn(fan_in, fan_out)/np.sqrt(fan_in/2)
```

---

# Dropuut

## overfiting을 막기위해 Dropout을 하자

    NN이 Deep해지면 overfiting이 일어나기 쉽다.

```python
# train my model
for epoch in range(training_epochs):
    avg_cost = 0
    total_batch = int(mnist.train.num_examples / batch_size)

    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        feed_dict = {X: batch_xs, Y: batch_ys}
        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)
        avg_cost += c / total_batch
```

    이 코드가 모형을 학습하는 과정인데
    Tensorflow에서 dropout을 간단하게 구현해준다.
    `feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}`
    `keep_prob: 0.7`

```python
keep_prob = tf.placeholder(tf.float32)
```

    이렇게 `placeholder`에 담아주고

```python
L1 = tf.nn.dropout(L1, keep_prob = keep_prob)
```

    layer에 `tf.nn.dropout`함수를 이용해서 dropout을 해준다.

```python
W1 = tf.get_variable("W1", shape=[784, 512],
                     initializer=tf.contrib.layers.xavier_initializer())
b1 = tf.Variable(tf.random_normal([512]))
L1 = tf.nn.relu(tf.matmul(X, W1) + b1)
L1 = tf.nn.dropout(L1, keep_prob=keep_prob)

W2 = tf.get_variable("W2", shape=[512, 512],
                     initializer=tf.contrib.layers.xavier_initializer())
b2 = tf.Variable(tf.random_normal([512]))
L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)
L2 = tf.nn.dropout(L2, keep_prob=keep_prob)
```
    
```python
# train my model
for epoch in range(training_epochs):
    avg_cost = 0
    total_batch = int(mnist.train.num_examples / batch_size)

    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}
        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)
        avg_cost += c / total_batch
```

    Dropout은 모형을 학습할 때만 사용한다.
    
    `sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1})`
    
    이렇게 Test데이터를 이용해서 예측혁을 판단할 때는 dropout을 하지 않는다.


# Optimizers

## cost를 줄일 때 GradientDescent 만 있는게 아니다.

    * sgd (stocastic gradient descent)
    * momentum
    * nag
    * adagrad
    * adadelta
    * rmsprop
    * Adam

    여러 optimizers 중에서 Adam이 가장 좋다더다.
    처음 시작할때 바로 adam을 사용하자.

```python
optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-3).minimize(cost)
#
optimizer = tf.train.AdamOptimizer(learning_rate = 1e-3).minimize(cost)
```



